{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.dirichlet([.5, .5, .5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = np.array([[0, 0], [1, 0], [0.5, 0.75**0.5]])\n",
    "AREA = 0.5 * 1 * 0.75**0.5\n",
    "triangle = tri.Triangulation(corners[:, 0], corners[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "refiner = tri.UniformTriRefiner(triangle)\n",
    "trimesh = refiner.refine_triangulation(subdiv=4)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for (i, mesh) in enumerate((triangle, trimesh)):\n",
    "    plt.subplot(1, 2, i+ 1)\n",
    "    plt.triplot(mesh)\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each corner of the triangle, the pair of other corners\n",
    "pairs = [corners[np.roll(range(3), -i)[1:]] for i in range(3)]\n",
    "# The area of the triangle formed by point xy and another pair or points\n",
    "tri_area = lambda xy, pair: 0.5 * np.linalg.norm(np.cross(*(pair - xy)))\n",
    "\n",
    "def xy2bc(xy):\n",
    "    '''Converts 2D Cartesian coordinates to barycentric.'''\n",
    "    coords = np.array([tri_area(xy, p) for p in pairs]) / AREA\n",
    "    \n",
    "    def validate_coord(coord):\n",
    "        eps = 1e-5\n",
    "        coord = [x if x > 0 else x + eps for x in coord]\n",
    "        coord = [min(x, 1.0) for x in coord]\n",
    "        \n",
    "        if sum(coord) != 1:\n",
    "            correction = 1 - sum(coord)\n",
    "            coord[np.argmax(coord)] = coord[np.argmax(coord)] + correction\n",
    "        \n",
    "        return coord\n",
    "    \n",
    "    coords = np.apply_along_axis(validate_coord, 0, coords)\n",
    "      \n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_pdf_contours(dist, nlevels=200, subdiv=6, **kwargs):\n",
    "\n",
    "    refiner = tri.UniformTriRefiner(triangle)\n",
    "    trimesh = refiner.refine_triangulation(subdiv=subdiv)\n",
    "    pvals = [dist.pdf(xy2bc(xy)) for xy in zip(trimesh.x, trimesh.y)]\n",
    "\n",
    "    plt.tricontourf(trimesh, pvals, nlevels, cmap='viridis', **kwargs)\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 0.75**0.5)\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = stats.dirichlet([3, 3, 2])\n",
    "draw_pdf_contours(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model(input_length=16, n_dense_nodes=16, output_size=3):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(n_dense_nodes, use_bias=False, input_shape=(input_length,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    assert model.output_shape == (None, 16)\n",
    "    \n",
    "    model.add(layers.Dense(n_dense_nodes, use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    assert model.output_shape == (None, 16)\n",
    "    model.add(layers.Dense(output_size, use_bias=False))\n",
    "    \n",
    "    assert model.output_shape == (None, 3)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "z = tf.random.normal([1, 16])\n",
    "\n",
    "generated_sample = generator(z, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model(input_length=3, n_dense_nodes=32):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(n_dense_nodes, use_bias=False, input_shape=(input_length,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    assert model.output_shape == (None, n_dense_nodes)\n",
    "    \n",
    "    model.add(layers.Dense(n_dense_nodes, use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    assert model.output_shape == (None, n_dense_nodes)\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision = discriminator(generated_sample)\n",
    "print(decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 16\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "loss_trace = {\n",
    "    \"generator_loss\": [],\n",
    "    \"discriminator_loss\": []\n",
    "}\n",
    "\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    loss_trace[\"generator_loss\"].append(gen_loss)\n",
    "    loss_trace[\"discriminator_loss\"].append(disc_loss)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # TODO: generate samples from generator.\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # TODO: generate samples from generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = dist.rvs(60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 60000\n",
    "train_samples = dist.rvs(BUFFER_SIZE)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_samples).shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-lawrence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_input():\n",
    "    return tf.random.normal([1, 16])\n",
    "\n",
    "z1 = get_generator_input()\n",
    "generator(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_inputs = [get_generator_input() for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_samples = [generator(z) for z in generator_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = [v[0] for v in generated_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_trace[\"discriminator_loss\"])\n",
    "plt.plot(loss_trace[\"generator_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-burden",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
